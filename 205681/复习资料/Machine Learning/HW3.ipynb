{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-UA.0473-â€‹001 Introduction to Machine Learning\n",
    "\n",
    "# Homework 3\n",
    "\n",
    "\n",
    "### Name: (your name goes here)\n",
    "\n",
    "\n",
    "### Due: Oct. 28, 2020\n",
    "\n",
    "\n",
    "## Goal:  The goal of this homework is to practice implementing kernels for support vector machine and to practice using cross-validation.\n",
    "\n",
    "Please DO NOT change the position of any cell in this assignment. If the notebook is stunned sometimes before the cell that defines SVM class, please restart it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages.\n",
    "\n",
    "## WARNING!\n",
    "Some parts below (especially with cross-validation) could take a while to run ~10-15 min.  If it takes much longer than this, then you likely have an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and spliting data (nothing to do here)\n",
    "For this assignment, we utilized the iris dataset from sklearn library. Most of the setting is the same as homework 2, except the training/test are split in $70\\%/30\\%$.\n",
    "* `X_train` and `y_train` are `features` and `labels` for training, while `X_test` and `y_test` are for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2] # Only use the first two features.\n",
    "y = y[y != 0]     # Ignore the first class.\n",
    "y[y==2] = -1      # Change class label to -1 for SVM.\n",
    "\n",
    "n_sample = len(X) # Total number of data points.\n",
    "\n",
    "# Split data into training and testing sets.\n",
    "#np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(np.float)\n",
    "\n",
    "X_train = X[:int(.7 * n_sample)]\n",
    "y_train = y[:int(.7 * n_sample)]\n",
    "X_test = X[int(.7 * n_sample):]\n",
    "y_test = y[int(.7 * n_sample):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernel SVM (75 pts total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (15 pts) Implementing kernel functions\n",
    "Recall that in homework 2 we only used the inner product $\\langle x_1,x_2\\rangle$ as the kernel. This time we are going implement three new kernels:\n",
    "\n",
    "1. `linear`: $k(x_1,x_2)=x_1^T x_2$.  Note that this is what you did in the previous homework and is already done for you.\n",
    "\n",
    "2. `poly`: $k(x_1,x_2)=\\left(x_1^T x_2 + 1\\right)^2$\n",
    "\n",
    "3. `rbf`: $k(x_1,x_2) = \\exp\\left( -\\frac{1}{2}\\|x_1 - x_2\\|^2 \\right)$.\n",
    "\n",
    "4. `laplace`: $k(x_1,x_2)=\\exp\\left(-\\frac{1}{2}\\left\\| x_1 - x_2 \\right\\| \\right)$\n",
    "\n",
    "Complete the function below by implementing the kernel function for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_product(x1, x2, kernel = 'linear'):\n",
    "    \"\"\"\n",
    "    Compute the kernelal product k(x1,x2) for different choices of the kernel k.\n",
    "    \n",
    "    Input:\n",
    "        x1: np.ndarray(p,), the first vector\n",
    "        x2: np.ndarray(p,), the second vector\n",
    "        kernel: str, a string which is the name of the kernel, must match one of the options below exactly\n",
    "        linear, poly, rbf, laplace\n",
    "    \n",
    "    Return:\n",
    "        k: float, the value of the kernel k(x1, x2)\n",
    "    \"\"\"\n",
    "    \n",
    "    ##TODO-start##\n",
    "    if kernel == 'linear':\n",
    "        k = np.dot(x1, x2)\n",
    "        return k\n",
    "    elif kernel == 'poly':\n",
    "        k = ?\n",
    "        return k\n",
    "    elif kernel == 'rbf':\n",
    "        k = ?\n",
    "        return k\n",
    "    elif kernel == 'laplace':\n",
    "        k = ?\n",
    "        return k\n",
    "    ## TODO-end##\n",
    "    else:\n",
    "        print(\"Invalid kernel: {:s}\".format(kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just a helper method similar to the previous homework.  You do not need to implement anything in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_product_matrix(X, kernel = 'linear'):\n",
    "    \"\"\"\n",
    "    Compute the inner product matrix of two vectors.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n,p), data matrix with n data points (each row) and p features\n",
    "        kernel: str, a string which is the name of the kernel, must match one of the options below exactly\n",
    "        linear, poly, rbf, laplace\n",
    "    \n",
    "    Return:\n",
    "        K: np.ndarray(n,n), each entry is the kernel product of the corresponding pair of vectors\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    K = np.zeros((n, n))  \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            K[i, j] = kernel_product(X[i], X[j], kernel)\n",
    "            K[j, i] = K[i, j] # Matrix is symmetric so we can cut computation in half.          \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (25 pts) Finishing the kernel SVM implementation\n",
    "\n",
    "Now you will extend your SVM implementation from the previous homework by including kernels.  Finish the SVM class below by filling in the missing lines of code.  You need to do 3 things, all of which are very similar to the previous homework.\n",
    "\n",
    "1. Implement the objective function for the dual problem for minimization.  Note that we actually maximize the dual objective function, but in order to use the `minimize()` function from Sci-Py you will need to take the negative.\n",
    "\n",
    "2. Compute the bias.  Use your implementation from the previous homework for guidance and think carefully about what needs to change.\n",
    "\n",
    "3. Implement the `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    \n",
    "    def __init__(self, C = 1, kernel = 'linear'):\n",
    "        \"\"\"\n",
    "        Initialize the SVM model.\n",
    "        \n",
    "        Input:\n",
    "            C: float, the regularization constant for SVM.\n",
    "            kernel: str, a string which is the name of the kernel, must match one of the options below exactly\n",
    "            linear, poly, rbf, laplace\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        assert C >= 0\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        # The following variables are set after fit() is called.\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.bias  = None\n",
    "        self.alpha = None\n",
    "     \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Computes the parameters alpha and bias that determine the maximum-margin decision boundary for SVM.\n",
    "        bias will be a float, alpha is a np.ndarray(n,) vector of the dual variables.\n",
    "    \n",
    "        Input:\n",
    "            X_train: np.ndarray(n,p), matrix of training data features\n",
    "            y_train: np.ndarray(n, ), vector of training data labels\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Save the training data.\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        # Number of training points and dimension.\n",
    "        n, p = X_train.shape\n",
    "        \n",
    "        # Get the negative objective function to change maximization to minimization.\n",
    "        ##TODO-start##\n",
    "        W = ?\n",
    "        ##TODO-end##\n",
    "                                                     \n",
    "        # Initialization and constraints for optimization.\n",
    "        init_pt = np.zeros(n)\n",
    "        bnds = tuple([(0, self.C) for i in range(n)])\n",
    "        cons = ({'type': 'eq', 'fun': lambda x:  np.dot(x, y_train)})\n",
    "        # Solve the dual problem for SVM.\n",
    "        res = minimize(W, init_pt, method='SLSQP', bounds=bnds,\n",
    "               constraints=cons)   \n",
    "        alpha = res.x\n",
    "        self.alpha = alpha\n",
    "                                                     \n",
    "        # Compute the bias\n",
    "        ##TODO-start##\n",
    "        self.bias = ?\n",
    "        ##TODO-end##\n",
    "    \n",
    "                                                     \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Compute the predictions y_pred on the test set using only the support vectors.\n",
    "    \n",
    "        Input:\n",
    "            X_test: np.ndarray(n,p), matrix of the test data\n",
    "    \n",
    "        Return:\n",
    "            y_pred: np.ndarray(n,), vector of the predicted labels, either +1 or -1\n",
    "        \"\"\"\n",
    "        ##TODO-start##\n",
    "        y_pred = ?\n",
    "        ##TODO-end##\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is taken from the previous homework.  There is nothing for you to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Computes the accuracy on the test set given the class predictions.\n",
    "    \n",
    "    Input:\n",
    "        y_pred: np.ndarray(n,), vector of predicted class labels\n",
    "        y_test: np.ndarray(n,), vector of true class labels\n",
    "    \n",
    "    Output:\n",
    "        float, accuracy of predictions\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred*y_test > 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the decision boundaries of all kernels (nothing to do here)\n",
    "The following is an illustration of decision boundaries for SVM with kernels. You do not need to do anything in the next cell.  It is only to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_list = ['linear', 'rbf', 'poly', 'laplace']\n",
    "\n",
    "for kernel in kernel_list:\n",
    "    model = SVM(C=10, kernel=kernel)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"SVM with {:s} kernel, accuracy = {:0.2f}%\".format(kernel, 100*accuracy(y_pred, y_test)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolor='k', s=20)\n",
    "\n",
    "    # Circle out the test data\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "                zorder=10, edgecolor='k')\n",
    "\n",
    "    plt.axis('tight')\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    XXYY = np.c_[XX.ravel(), YY.ravel()]\n",
    "    Z = model.predict(XXYY)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.title(kernel)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (20 pts) Cross-validation to choose the regularization parameter $C$\n",
    "\n",
    "Complete the cross-validation function below.  The data has already been randomly arranged for you.  There are 4 things you will need to do for each fold.\n",
    "\n",
    "1. Split the data.\n",
    "2. Train the model.\n",
    "3. Get the predictions.\n",
    "4. Compute the accuracy.\n",
    "\n",
    "Once you have finished run the following cell to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, X_train, y_train, folds = 5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on model using the available training data.  You may assume that \n",
    "    the number of training data points is divisible by the number of folds.\n",
    "    \n",
    "    Input:\n",
    "        model: SVM object, an instance of the SVM class, must have fit and predict implemented.\n",
    "        X_train: np.ndarray(n,p), training data features\n",
    "        y_train: np.ndarray(n,), training data labels\n",
    "        folds: int, number of cross-validation folds to perform\n",
    "    \n",
    "    Output:\n",
    "        acc: float, the mean accuracy from all cross-validation folds\n",
    "        acc_results: np.ndarray(folds,), the accuracy results from each cross-validation fold\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0] # Number of available training data points.\n",
    "    acc_results = np.zeros(folds) # Store the cross-validation results here.\n",
    "    \n",
    "    # Randomly permute the data.\n",
    "    permutation = np.random.permutation(n)\n",
    "    X_train = X_train[permutation, :]\n",
    "    y_train = y_train[permutation]\n",
    "    \n",
    "    for k in range(folds):\n",
    "        print(\"Fold {:d} / {:d} is running.\".format(k, folds))\n",
    "        \n",
    "        ##TODO-start##\n",
    "\n",
    "        ##TODO-end##\n",
    "    \n",
    "    acc = np.mean(acc_results)\n",
    "    return (acc, acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to check your work.  There is nothing you need to implement here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [0.001, 0.01, 0.1, 0.5, 1, 5]\n",
    "\n",
    "for c in C_list:\n",
    "    print('Cross-validation for C = {:0.3f}'.format(c))\n",
    "    model = SVM(C = c, kernel = 'rbf')\n",
    "    acc, acc_results = cross_validation(model, X_train, y_train)\n",
    "    print('Mean = {:0.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) (10 pts) Mean and standard deviation of cross-validation scores\n",
    "\n",
    "Now use 10-fold cross-validation on two models: 1. SVM with RBF kernel and 2. SVM with the linear kernel.  Set the regularization parameter $C = 0.5$ for both models.  Print out the mean accuracy and the standard deviation of the accuracies from each fold using the numpy functions `np.mean()` and `np.std()`.  Format your results as percentages to two decimal places using Python's `format()` function for strings so that it is easy to read.  Make sure the values you are printing are clearly indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO-start##\n",
    "print('Cross-validation for SVM with RBF kernel')\n",
    "\n",
    "\n",
    "print('Cross-validation for SVM with linear kernel')\n",
    "\n",
    "##TODO-end##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) (10 pts) Generalization error\n",
    "\n",
    "Remember that the reason for why we do cross-validation is because we want an estimate of the generalization error of our model.\n",
    "$$\n",
    "    E = \\mathbb{E}_{(X,Y)}\\left[ \\mathbf{1}\\{ f(X) \\neq Y \\} \\right]\n",
    "$$\n",
    "where the indicator function\n",
    "$$\n",
    "    \\mathbf{1}\\{ f(X) \\neq Y \\} = \\begin{cases} \n",
    "    1 & f(X) \\neq Y\\\\\n",
    "    0 & f(X) = Y\n",
    "    \\end{cases}\n",
    "$$\n",
    "and $f(X)$ is our prediction for the test point $X$.  Remember that the expecation is taken over the joint distribution of $(X,Y)$.  Since we only have $n$ training data pairs $\\{(x_i, y_i)\\}_{i=1}^n$ we approximate the expecation with a sample average\n",
    "$$\n",
    "    \\hat{E}_n = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ f(x_i) \\neq y_i \\}\n",
    "$$\n",
    "Note that $f$ itself is learned (fitted) using the training points.  Suppose that we have $n$ available data points $\\{(x_i,y_i)\\}_{i=1}^n$ and that we split the data into a training set $D_{\\text{train}} = \\{(x_j, y_j)\\}_{j=1}^{n_{\\text{train}}}$ of $n_{\\text{train}}$ points as well as a validation set $D_{\\text{val}} = \\{(x_i, y_i)\\}_{i=n_{\\text{train}} + 1}^{n_{\\text{train}} + n_{\\text{val}}}$ of $n_{\\text{val}}$ points with $n_{\\text{val}} + n_{\\text{train}} = n$.  Now answer the following two questions.\n",
    "\n",
    "\n",
    "1. Is it true that if $n_{\\text{val}} \\to \\infty$ then $\\hat{E}_{n_{\\text{val}}} \\to E$?  Here $\\hat{E}_{n_{\\text{val}}}$ is computed using only the validation points.  You do not need a formal proof, but provide a clear explanation.  Hint: you may want to remind yourself of the law of large numbers.\n",
    "\n",
    "2. Give a heuristic explanation as to why $\\hat{E}_{n_{\\text{val}}}$ is a better estimator of $E$ than $\\hat{E}_{n_{\\text{train}}}$ (error computed only training points) assuming that $n_{\\text{val}} = n_{\\text{train}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding a feature map (15 pts total)\n",
    "\n",
    "Consider the following dataset shown below, which is just two concentric circles centered at $0$ of radius 1 and 2 corresponding to the two different classes.  There is nothing for you to implement in the cell below.  It is only for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the toy dataset which is just two circles of radius 1 and 2.\n",
    "\n",
    "theta = np.linspace(0, 2*np.pi, 20) # Angles.\n",
    "\n",
    "# Class 0\n",
    "x0 = np.cos(theta)\n",
    "y0 = np.sin(theta)\n",
    "\n",
    "# Class 1\n",
    "x1 = 2*np.cos(theta)\n",
    "y1 = 2*np.sin(theta)\n",
    "\n",
    "# Plot the data.\n",
    "plt.figure()\n",
    "plt.plot(x0, y0, 'b x', label = 'Class 0')\n",
    "plt.plot(x1, y1, 'r s', label = 'Class 1')\n",
    "plt.axis('equal')\n",
    "plt.title('Toy Dataset')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we should be able to easily separate this data to get a classifier with $100\\%$ accuracy since there is no overlap.  Answer the following questions.\n",
    "\n",
    "1.  Write down a feature map $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}$, such that the transformed data $\\phi(x)$ is linearly separable.  \n",
    "\n",
    "2.  What is the corresponding kernel?\n",
    "\n",
    "3.  What is the decision boundary on the original space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verifying the Polynomial Kernel (10 pts total)\n",
    "\n",
    "Prove that the polynomial kernel $K(u,v) = (u^Tv + 1)^d$ for $d \\ge 1$ and $d$ integer is a valid kernel.  This means you need to verify the assumptions of Mercer's theorem that\n",
    "\n",
    "1. $K$ is symmetric.\n",
    "\n",
    "2. $K$ is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "331px",
    "left": "710px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
