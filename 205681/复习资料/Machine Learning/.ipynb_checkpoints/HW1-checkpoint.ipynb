{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-UA.0473-â€‹001 Introduction to Machine Learning\n",
    "\n",
    "# Homework 1\n",
    "\n",
    "\n",
    "### Name: Lejia Hu\n",
    "\n",
    "\n",
    "### Due: September 30, 2020\n",
    "\n",
    "\n",
    "## Goal:  The goal of this homework is to practice implementing a logistic regression model and gradient descent as well as to explore some theoretical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autograd==1.3 in /Users/lejiahu/opt/anaconda3/lib/python3.8/site-packages (1.3)\n",
      "Requirement already satisfied: future>=0.15.2 in /Users/lejiahu/opt/anaconda3/lib/python3.8/site-packages (from autograd==1.3) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/lejiahu/opt/anaconda3/lib/python3.8/site-packages (from autograd==1.3) (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"autograd==1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Gradient Descent (30 pts total)\n",
    "\n",
    "In this problem you will implement gradient descent as a general purpose optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) Implementing gradient descent with a fixed learning rate\n",
    "\n",
    "Using autograd to compute the derivative, implement gradient descent with a fixed learning rate `lr` for a general scalar function `fun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a fixed number of iterations of gradient descent on a function using a fixed learning rate. \n",
    "\n",
    "Input:\n",
    "    fun : function handle: function that takes in a numpy array of shape (d,) and returns a float\n",
    "    x0  : initial point: numpy array of shape (d,)\n",
    "    lr  : fixed learning rate: a positive float\n",
    "    iterations : number of iterations to perform: int\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun: numpy array of shape (d,)\n",
    "\"\"\"\n",
    "def gd_fixed(fun, x0, lr, iterations):\n",
    "    #compute gradient module using autograd\n",
    "    gradient = grad(fun)\n",
    "    #run the gradient descent loop\n",
    "    for k in range(iterations):        \n",
    "        x0 = x0 - lr*gradient(x0)\n",
    "    #set the final x0 to x\n",
    "    x = x0    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Using a variable learning rate\n",
    "\n",
    "Sometimes it is necessary to decrease our learning rate as we iterate to help gradient descent converge.  Implement gradient descent below where the learning rate at iteration $i$ is given by\n",
    "$$\n",
    "\\mathrm{lr}_i = \\frac{\\mathrm{lr}}{i+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a fixed number of iterations of gradient descent on a function using a variable learning rate. \n",
    "\n",
    "Input:\n",
    "    fun : function handle: function that takes in a numpy array of shape (d,) and returns a float\n",
    "    x0  : initial point: numpy array of shape (d,)\n",
    "    lr  : initial learning rate: a positive float\n",
    "    iterations : number of iterations to perform: int\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun: numpy array of shape (d,)\n",
    "\"\"\"\n",
    "def gd_variable(fun, x0, lr, iterations):\n",
    "    # TO DO:\n",
    "    #compute gradient module using autograd\n",
    "    gradient = grad(fun)\n",
    "    #run the gradient descent loop\n",
    "    for i in range(iterations):         \n",
    "        lr_i = lr/(i+1) #change to variable learning rate \n",
    "        x0 = x0 - lr_i * gradient(x0)\n",
    "    #set the final x0 to x\n",
    "    x = x0    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (10 pts)  Choosing the learning rate\n",
    "\n",
    "Let $\\alpha$ denote the fixed learning rate and consider the function $f(x) = \\frac{1}{2}x^2$.  The gradient descent update rule at iteration $n+1$ is given by\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\alpha f'(x_n)\n",
    "$$\n",
    "\n",
    "Is there a critical value $\\alpha_0$ so that if $\\alpha \\ge \\alpha_0$ then gradient descent will not converge for $f(x)$?  If so what is it?  Give a proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "The derivation of $f(x)$ is $f'(x)=x$, and $x=0$ is the optimum of $f(x)$, the critical value $\\alpha_0 = 2$. <br>\n",
    "When $\\alpha_0 = 2$, $x_{n+1} = x_n - 2 * x_n = -x_n$, hence the sequence is alternating and $f(x_n) == f(x_{n+1})$. <br>\n",
    "When $\\alpha > 2$, $f(x_n) \\leq f(x_{n+1})$. With the iteration going by, $f(x_n)$ becomes bigger. So it cannot be converaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d)  (10 pts) Feature scaling\n",
    "\n",
    "Oftentimes it is advantageous to rescale or normalize our features.  As a toy example suppose we want to predict a person's weight (in kgs.) based on their height.  We would like an algorithm that gives equally good predictions whether height is measured in centimeters or kilometers.  For a concrete example, consider both of the following 2-dimensional optimization problems:\n",
    "$$\n",
    "\\mathrm{argmin}_{x\\in \\mathbb{R}^2}\\ x^T \\Sigma_i x, \\quad i = 1,2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Sigma_1 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}, \\quad \n",
    "\\Sigma_2 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 100\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Suppose our starting point is the same $x_0 = (1, 1)^T$ for both optimization problems.  For a fixed learning rate, will gradient descent perform better on problem $i=1$ or $i=2$?  Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "I think GD performs better on when i=1. The optimization problem is on 2-dimensional surface. With the i = 1, the gradient of two dimension is equal, causing the speed of decline to be even. But with the parameter i =2, the speed of decline in the two dimensions have a large difference, which would have an impact on the algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Logistic Regression (60 pts total)\n",
    "\n",
    "In this problem you will implement all of the steps that are taken care of whenever the `fit` function from the sci-kit learn package is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) Logisitc Unit\n",
    "\n",
    "For binary classification with label $y \\in \\{0,1\\}$ we can model the posterior probability $p(y = 1 | x)$ with the logistic unit \n",
    "$$\n",
    "h(x; w) = \\frac{1}{1 + \\exp(-w^Tx)} .\n",
    "$$\n",
    "We will use the convention that $x_0 = 1$.  Implement this function below using the skeletal outline as a guide.  Suppose we use a discriminant function $f(x)$ which assigns the label $y = 1$ if $p(y = 1 | x) \\ge 0.5$ and $0$ otherwise.  In other words, with a discriminant function we do not need the posterior probabilities but rather skip straight to the classification.  Implement the following method below to compute this quantity for every point in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the logistic unit h(x;w) on each point x in the dataset X (each row is a different point).  This code \n",
    "should be vectorized for efficient computation (i.e. no for loops).\n",
    "\n",
    "Input:\n",
    "    w: weight vector: numpy array of shape (d,) where d is the dimension\n",
    "    X: data: numpy array of shape (n, d) where n is the number of data points\n",
    "\n",
    "Return:\n",
    "    logits : h(x;w): a numpy array of shape (n,) that has the values \n",
    "\"\"\"\n",
    "def logistic_unit(X, w):\n",
    "    # TO DO:\n",
    "    h = 1 / (1 + np.exp(np.matmul(w, X.T)))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Discriminant function\n",
    "\n",
    "Give a geometric interpretation of our classifier once the parameters $w$ have been learned.  What does the angle between the vectors $w$ and $x$ tell us about the predicted class value?  For which angles do we predict $y=1$?  Write down the discriminant function $f(x)$ as simply as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:<br>\n",
    "As the angle between $w$ and $x$ approaches to 0, the dot product gets bigger, and thus ${1 + \\exp(-w^Tx)}$ converges to 0 as well, so $p(y=1)$ converges to 1, which means model has more confidence to predict the class to 1. <br>\n",
    "The angles between [0, $\\frac{\\pi}{2}$] and angles between [$\\frac{3\\pi}{4}$, 0] can make y=1. <br>\n",
    "$ f(x)=\\left\\{\n",
    "\\begin{aligned}\n",
    "1 \\quad p(y = 1 | x) \\ge 0.5; \\\\\n",
    "0 \\quad p(y = 1 | x) < 0.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (10 pts) Deriving the loss function\n",
    "\n",
    "We have implicitly made the assumption that $y|x \\sim \\mathrm{Bernoulli}(h(x;w))$.  If we have an iid dataset $\\{(x_i,y_i)\\}_{i=1}^N$, then we can write the data likelihood as\n",
    "$$\n",
    "p(\\vec{y}| X, w) = \\prod_{i=1}^N p(y_i | x_i, w)\n",
    "$$\n",
    "We can learn the parameter $w$ by maximizing this probability (i.e. we find the maximum likelihood estimator) or equivalently minimizing $J(w) = -\\log p(\\vec{y} | X, w)$.  Derive the loss function $J(w)$ step-by-step and implement it using the skeletal outline below.  In your implementation you may use the convention that $0 \\log 0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**: <br>\n",
    "Let $p(y_i|x_i, w) = p(y_i=1|x_i, w)^{y_i}(1 - p(y_i=1|x_i, w))^{1-y_i}$,\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        &\\max_{w}\\prod_{i=1}^N p(y_i | x_i, w) \\Leftrightarrow \\min_{w}\\left(-\\sum_{i=1}^N\\log{p(y_i|x_i, w)}\\right) \\\\\n",
    "        &=\\min_{w}\\left(-\\sum_{i=1}^Ny_i\\log{p(y_i=1|x_i, w)}+(1-y_i)\\log{(1-p(y_i=1|x_i, w))}\\right) \\\\\n",
    "        &=\\min_{w}\\left(-\\sum_{i=1}^Ny_i\\log{h(x_i; w)}+(1-y_i)\\log{(1-h(x_i; w)})\\right) \\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "therefore,\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        J(w) = -\\sum_{i=1}^Ny_i\\log{h(x_i; w)}+(1-y_i)\\log{(1-h(x_i; w)}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "then, we derive this function $J(w)$ as below,\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial{J(w)}}{\\partial{w}} &= -\\sum_{i=1}^Ny_i*(\\log{h(x_i; w))^\\prime}+(1-y_i)*(\\log{(1-h(x_i; w))})^\\prime \\\\\n",
    "        &=-\\sum_{i=1}^Ny_i*\\frac{h(x_i; w)^\\prime}{h(x_i; w)}+(1-y_i)*\\frac{-h(x_i; w)^\\prime}{1-h(x_i; w)}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "Because the derivation of sigmoid function to w is \n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial{h(x_i; w)}}{\\partial{w}} &= \\frac{\\exp{(-w^Tx_i)}}{(1+\\exp{(-w^Tx_i)})^2} * x_i \\\\\n",
    "        &=h(x_i; w)(1-h(x_i; w)) * x_i\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "so, we can get \n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial{J(w)}}{\\partial{w}} &=-\\sum_{i=1}^Ny_i*\\frac{h(x_i; w)^\\prime}{h(x_i; w)}+(1-y_i)*\\frac{-h(x_i; w)^\\prime}{1-h(x_i; w)} \\\\\n",
    "        &=-\\sum_{i=1}^Ny_i*\\frac{h(x_i; w)(1-h(x_i; w)) * x_i}{h(x_i; w)}+(1-y_i)*\\frac{-h(x_i; w)(1-h(x_i; w)) * x_i}{1-h(x_i; w)} \\\\\n",
    "        &=-\\sum_{i=1}^Nx_iy_i(1-h(x_i; w))-x_i(1-y_i)h(x_i;w) \\\\\n",
    "        &=-\\sum_{i=1}^Nx_i(y_i-h(x_i; w)) \\\\\n",
    "        &=\\sum_{i=1}^Nx_i(h(x_i; w)-y_i)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the loss function using the formula you derived.  This code does not need to be vectorized and you\n",
    "may use the logistic_unit function.\n",
    "\n",
    "Input:\n",
    "    w : weight vector: numpy array of shape (d+1,) (remember that x_0 = 1)\n",
    "    X : dataset features: numpy array of shape (n, d)\n",
    "    y : dataset targets: numpy array of shape (n,) contains only 0's or 1's\n",
    "\n",
    "Return:\n",
    "    J : loss of w given X,y: float\n",
    "\"\"\"\n",
    "def loss(w, X, y):\n",
    "    # TO DO:\n",
    "    h = logistic_unit(X, w)\n",
    "    J = -(np.sum(y * np.log(h)) + np.sum((1 - y) * np.log(1 - h)))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making data (Nothing to do here)\n",
    "\n",
    "The following method generates random data to test your algorithm on.  **DO NOT CHANGE THE METHOD!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following method generates a random dataset.  DO NOT ALTER THIS METHOD.\n",
    "def make_data(n_samples = 500):\n",
    "    # Generate data features.\n",
    "    X1 = mvn.rvs(mean = np.array([1.1, 0]), cov = 0.2*np.eye(2), size = n_samples//2)\n",
    "    X2 = mvn.rvs(mean = np.array([-1.1, 0]), cov = 0.2*np.eye(2), size = n_samples - n_samples//2)\n",
    "    # Append data labels and combine.\n",
    "    X1 = np.hstack( (X1, np.ones((X1.shape[0], 1))))\n",
    "    X2 = np.hstack( (X2, np.zeros((X2.shape[0], 1))))\n",
    "    X = np.vstack([X1, X2])\n",
    "    # Randomly permute data.\n",
    "    np.random.shuffle(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) (5 pts) Splitting the data for training and testing\n",
    "\n",
    "Now we'll actually learn a logistic regression model for some synthetic data.\n",
    "\n",
    "First split the dataset into a training, validation, and test set.  Use a 40/40/20 split (roughly 40/40/20 is fine).  You do not need to use a random splitting, although in practice it is usually a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first generate some fake data.\n",
    "np.random.seed(2) # Don't change the random seed.\n",
    "data = make_data()\n",
    "X = data[:,:-1] # Features\n",
    "y = data[:,-1]  # Labels\n",
    "\n",
    "# We'll also augment the data so that x_0 = 1 for the intercept term.\n",
    "X = np.append(np.ones((len(X), 1)), X, axis = 1)\n",
    "\n",
    "\n",
    "# TO DO:\n",
    "## Your part starts here.\n",
    "size = X.shape[0]\n",
    "X_train= X[: int(0.4 * size), :]\n",
    "y_train = y[: int(0.4 * size)]\n",
    "X_valid = X[int(0.4 * size): int(0.8 * size), :]\n",
    "y_valid = y[int(0.4 * size): int(0.8 * size)]\n",
    "X_test = X[int(0.8 * size):, :]\n",
    "y_test = y[int(0.8 * size):]\n",
    "## Ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) (20 pts)  Training the model\n",
    "\n",
    "Now use your gradient descent function to learn the parameters $w$ using `5000` iterations.  You may choose the learning rate and initial parameters $w_0$ for this problem.  Compare both the fixed learning rate and variable learning rate gradient descents side by side (i.e. 2 subplots) by computing the loss after every `m=10` iterations.  There are 4 things to do for this problem.\n",
    "1. Set the learning rate and initial points.\n",
    "2. Implement the loss function $J(w)$.\n",
    "3. Update the parameters using the two gradient descent methods.\n",
    "4. Compute the loss after every 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = 5000        # Lots of iterations to see the training error go down.\n",
    "m = 10                   # Get the loss every m iterations.\n",
    "\n",
    "# TO DO: Set the learning rate.\n",
    "lr1 = 0.001              # You choose this value, try something small.\n",
    "lr2 = 0.001\n",
    "\n",
    "\n",
    "## Store the loss values in these arrays.\n",
    "# Fixed learning rate.\n",
    "train_loss_fixed = np.zeros(iterations//m)\n",
    "val_loss_fixed = np.zeros(iterations//m)\n",
    "\n",
    "# Variable learning rate.\n",
    "train_loss_var = np.zeros(iterations//m)\n",
    "val_loss_var = np.zeros(iterations//m)\n",
    "\n",
    "\n",
    "# TO DO: Set the initial values, either randomly or some fixed value.\n",
    "w1 = np.random.rand(3)\n",
    "w2 = w1\n",
    "\n",
    "# TO DO: Write the loss function as a function of the parameter only.\n",
    "J = lambda w: loss(w, X_train, y_train)\n",
    "grad_J = grad(J)\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TO DO: Get the updated parameters from gradient descent with a fixed learning rate.\n",
    "    w1 = w1 - lr1 * grad_J(w1)\n",
    "    # TO DO: Get the updated parameters from gradient descent with a variable learning rate.\n",
    "    # Hint: You may either modify the gd_variable function above or call gd_fixed with a different learning rate.\n",
    "    lr_2 = lr2/(i+1)\n",
    "    w2 = w2 - lr_2 * grad_J(w2)\n",
    "    \n",
    "    # Only compute the loss every m iterations.\n",
    "    if np.mod(i, m) == 0: \n",
    "        # TO DO: Compute the training and validation loss of the parameters found with the fixed learning rate.\n",
    "        train_loss_fixed[i//m] = loss(w1, X_train, y_train)\n",
    "        val_loss_fixed[i//m] = loss(w1, X_valid, y_valid)\n",
    "        # TO DO: Compute the training and validation loss of the parameters found with a variable learning rate.\n",
    "        train_loss_var[i//m] = loss(w2, X_train, y_train)\n",
    "        val_loss_var[i//m] = loss(w2, X_valid, y_valid)\n",
    "\n",
    "        \n",
    "## Plotting starts here.  Nothing to implement.\n",
    "its = np.arange(1, iterations + 1, m)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 4))\n",
    "\n",
    "ax1.semilogy(its, train_loss_fixed, 'b-', label = 'Train')\n",
    "ax1.semilogy(its, val_loss_fixed, 'r-', label = 'Val')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('GD with Fixed Learning Rate')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.semilogy(its, train_loss_var, 'b-', label = 'Train')\n",
    "ax2.semilogy(its, val_loss_var, 'r-', label = 'Val')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('GD with Variable Learning Rate')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) (10 pts) Evaluating the model on the test set\n",
    "\n",
    "Finally, we have trained our models and are ready to evaluate them on the test set.  For binary classification one way to check our classifier is to make a *confusion matrix* of our predictions.\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "\\text{Predict 0, Actual 0} & \\text{Predict 0, Actual 1}\\\\\n",
    "\\text{Predict 1, Actual 0} & \\text{Predict 1, Actual 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The diagonal elements are the number of samples that are correctly classified.\n",
    "\n",
    "Use both trained models (fixed and variable learning rates) to classify samples in the test set according to whether $h(x_i; w) \\ge 0.5$ or not and print the confusion matrices.  Also print the accuracy rate which is just the percentage of correctly classified examples for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_1 = \n",
      "[[51.  3.]\n",
      " [ 1. 45.]]\n",
      "C_2 = \n",
      "[[52. 12.]\n",
      " [ 0. 36.]]\n",
      "Fixed Learning Rate Accuracy    = 96.0%\n",
      "Variable Learning Rate Accuracy = 88.0%\n"
     ]
    }
   ],
   "source": [
    "C1 = np.zeros((2, 2)) # Confusion matrix for fixed learning rate.\n",
    "C2 = np.zeros((2, 2)) # Confusion matrix for variable learning rate.\n",
    "N = len(X_test)       # Number of test samples.\n",
    "\n",
    "\n",
    "## TO DO STARTS HERE:\n",
    "pred_y1 = (logistic_unit(X_test ,w1) >= 0.5).astype(np.int16)\n",
    "pred_y2 = (logistic_unit(X_test ,w2) >= 0.5).astype(np.int16)\n",
    "C1[0][0] = np.logical_and(pred_y1 == y_test, y_test == 0).sum()\n",
    "C1[1][1] = np.logical_and(pred_y1 == y_test, y_test == 1).sum()\n",
    "C1[0][1] = np.logical_and(pred_y1 != y_test, y_test == 1).sum()\n",
    "C1[1][0] = np.logical_and(pred_y1 != y_test, y_test == 0).sum()\n",
    "\n",
    "C2[0][0] = np.logical_and(pred_y2 == y_test, y_test == 0).sum()\n",
    "C2[1][1] = np.logical_and(pred_y2 == y_test, y_test == 1).sum()\n",
    "C2[0][1] = np.logical_and(pred_y2 != y_test, y_test == 1).sum()\n",
    "C2[1][0] = np.logical_and(pred_y2 != y_test, y_test == 0).sum()\n",
    "# Compute the accuracy         \n",
    "acc1 = (C1[0][0] + C1[1][1]) / np.sum(C1)\n",
    "acc2 = (C2[0][0] + C2[1][1]) / np.sum(C2)\n",
    "\n",
    "# TO DO ENDS HERE.\n",
    "print(\"C_1 = \")\n",
    "print(C1)\n",
    "print(\"C_2 = \")\n",
    "print(C2)\n",
    "print('Fixed Learning Rate Accuracy    = {:0.1f}%'.format(100 * acc1))\n",
    "print('Variable Learning Rate Accuracy = {:0.1f}%'.format(100 * acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Loss functions (10 pts total)\n",
    "\n",
    "We have already seen two examples of loss functions: the squared loss and the logistic loss.  In this problem we will look at two more important examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) $L^p$ loss\n",
    "\n",
    "Similar to the squared loss, we can minimize the $L^p$ loss defined as\n",
    "$$\n",
    "L(w) = \\frac{1}{n} \\sum_{i=1}^n |y_i - w^T x_i |^p\n",
    "$$\n",
    "for $p > 1$.  Whenever, $p = 1$ the problem is no longer differentiable so we cannot take the gradient.  Derive the gradient of this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial L(w)}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^Ng(x_i, y_i, w); \\ \\ \\\n",
    "        g(x_i, y_i, w)=\\left\\{\n",
    "            \\begin{aligned}\n",
    "                &p(y_i - w^T x_i)^{p-1}(-x_i) \\quad y_i \\ge w^T x_i; \\\\\n",
    "                &p(w^T x_i - y_i)^{p-1}(x_i) \\quad y_i < w^T x_i\n",
    "            \\end{aligned}\n",
    "        \\right.\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (5 pts) Regularized loss\n",
    "\n",
    "Sometimes it's a good idea to add an additional penalty to the weights.  An example, which appears in ridge regression, is\n",
    "$$\n",
    "L(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i - w^T x_i)^2 + \\sum_{j=1}^d w_j^2\n",
    "$$\n",
    "Derive the gradient of this loss function.  How does the minimizer $w$ of the regularized loss compare to the minimzer of the squared loss?  (A heuristic explanation is fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial L(w)}{\\partial w} = \\frac{2}{n}\\sum_{i=1}^N(w^Tx_i-y_i)x_i + 2\\sum_{j=1}^dw_j\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "<br>\n",
    "When adding the item $\\sum_{j=1}^d w_j^2 \\ $, the parameter $w$ wouldn't be bigger because from the geomtric perspective $w^2$ forms a circle with center at origin and radius = 1, so the parameter $w$ would not exceed 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
