{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-UA.0473-​001 Introduction to Machine Learning\n",
    "\n",
    "# Homework 1\n",
    "\n",
    "\n",
    "### Name: (your name goes here)     Lejia Hu\n",
    "\n",
    "### Due: September 30, 2020\n",
    "\n",
    "\n",
    "## Goal:  The goal of this homework is to practice implementing a logistic regression model and gradient descent as well as to explore some theoretical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Gradient Descent (30 pts total)\n",
    "\n",
    "In this problem you will implement gradient descent as a general purpose optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) Implementing gradient descent with a fixed learning rate\n",
    "\n",
    "Using autograd to compute the derivative, implement gradient descent with a fixed learning rate `lr` for a general scalar function `fun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a fixed number of iterations of gradient descent on a function using a fixed learning rate. \n",
    "\n",
    "Input:\n",
    "    fun : function handle: function that takes in a numpy array of shape (d,) and returns a float\n",
    "    x0  : initial point: numpy array of shape (d,)\n",
    "    lr  : fixed learning rate: a positive float\n",
    "    iterations : number of iterations to perform: int\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun: numpy array of shape (d,)\n",
    "\"\"\"\n",
    "def gd_fixed(fun, x0, lr, iterations):\n",
    "    #compute gradient module using autograd\n",
    "    gradient = grad(fun)\n",
    "    #run the gradient descent loop\n",
    "    for k in range(iterations):        \n",
    "        x0 = x0 - lr*gradient(x0)\n",
    "    #set the final x0 to x\n",
    "    x = x0    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Using a variable learning rate\n",
    "\n",
    "Sometimes it is necessary to decrease our learning rate as we iterate to help gradient descent converge.  Implement gradient descent below where the learning rate at iteration $i$ is given by\n",
    "$$\n",
    "\\mathrm{lr}_i = \\frac{\\mathrm{lr}}{i+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a fixed number of iterations of gradient descent on a function using a variable learning rate. \n",
    "\n",
    "Input:\n",
    "    fun : function handle: function that takes in a numpy array of shape (d,) and returns a float\n",
    "    x0  : initial point: numpy array of shape (d,)\n",
    "    lr  : initial learning rate: a positive float\n",
    "    iterations : number of iterations to perform: int\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun: numpy array of shape (d,)\n",
    "\"\"\"\n",
    "def gd_variable(fun, x0, lr, iterations):\n",
    "    \n",
    "    #compute gradient module using autograd\n",
    "    gradient = grad(fun)\n",
    "    #run the gradient descent loop\n",
    "    for i in range(iterations):\n",
    "         gradient(x0)\n",
    "        lr_i = lr/(i+1) #change to variable learning rate \n",
    "        x0 = x0 - lr_i*grad_eval\n",
    "    #set the final x0 to x\n",
    "    x = x0    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (10 pts)  Choosing the learning rate\n",
    "\n",
    "Let $\\alpha$ denote the fixed learning rate and consider the function $f(x) = \\frac{1}{2}x^2$.  The gradient descent update rule at iteration $n+1$ is given by\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\alpha f'(x_n)\n",
    "$$\n",
    "\n",
    "Is there a critical value $\\alpha_0$ so that if $\\alpha \\ge \\alpha_0$ then gradient descent will not converge for $f(x)$?  If so what is it?  Give a proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "#Gradient descent converges if α < 1/L, where L is the Lipchitz constant. \n",
    "#For this function f(x), f'(x)= x, and L = sup(abs(f'(x))) = infinity \n",
    "#Hence, when α >= 0, then gradient descent will not converge. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d)  (10 pts) Feature scaling\n",
    "\n",
    "Oftentimes it is advantageous to rescale or normalize our features.  As a toy example suppose we want to predict a person's weight (in kgs.) based on their height.  We would like an algorithm that gives equally good predictions whether height is measured in centimeters or kilometers.  For a concrete example, consider both of the following 2-dimensional optimization problems:\n",
    "$$\n",
    "\\mathrm{argmin}_{x\\in \\mathbb{R}^2}\\ x^T \\Sigma_i x, \\quad i = 1,2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Sigma_1 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}, \\quad \n",
    "\\Sigma_2 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 100\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Suppose our starting point is the same $x_0 = (1, 1)^T$ for both optimization problems.  For a fixed learning rate, will gradient descent perform better on problem $i=1$ or $i=2$?  Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "It would perform better when i =2, because it is sufficiently large for GD to give a clear prediction. However, when i=1, the results might bounce around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Logistic Regression (60 pts total)\n",
    "\n",
    "In this problem you will implement all of the steps that are taken care of whenever the `fit` function from the sci-kit learn package is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) Logisitc Unit\n",
    "\n",
    "For binary classification with label $y \\in \\{0,1\\}$ we can model the posterior probability $p(y = 1 | x)$ with the logistic unit \n",
    "$$\n",
    "h(x; w) = \\frac{1}{1 + \\exp(-w^Tx)} .\n",
    "$$\n",
    "We will use the convention that $x_0 = 1$.  Implement this function below using the skeletal outline as a guide.  Suppose we use a discriminant function $f(x)$ which assigns the label $y = 1$ if $p(y = 1 | x) \\ge 0.5$ and $0$ otherwise.  In other words, with a discriminant function we do not need the posterior probabilities but rather skip straight to the classification.  Implement the following method below to compute this quantity for every point in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the logistic unit h(x;w) on each point x in the dataset X (each row is a different point).  This code \n",
    "should be vectorized for efficient computation (i.e. no for loops).\n",
    "\n",
    "Input:\n",
    "    w: weight vector: numpy array of shape (d,) where d is the dimension\n",
    "    X: data: numpy array of shape (n,d) where n is the number of data points\n",
    "\n",
    "Return:\n",
    "    logits : h(x;w): a numpy array of shape (n,) that has the values \n",
    "\"\"\"\n",
    "def sigmoid(X):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def net_input(w, X):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def logistic_unit(X, w):\n",
    "    # TO DO:\n",
    "    h = sigmoid(net_input(w,X))\n",
    "    return h\n",
    "    # Returns the probability after passing through sigmoid\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Discriminant function\n",
    "\n",
    "Give a geometric interpretation of our classifier once the parameters $w$ have been learned.  What does the angle between the vectors $w$ and $x$ tell us about the predicted class value?  For which angles do we predict $y=1$?  Write down the discriminant function $f(x)$ as simply as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (10 pts) Deriving the loss function\n",
    "\n",
    "We have implicitly made the assumption that $y|x \\sim \\mathrm{Bernoulli}(h(x;w))$.  If we have an iid dataset $\\{(x_i,y_i)\\}_{i=1}^N$, then we can write the data likelihood as\n",
    "$$\n",
    "p(\\vec{y}| X, w) = \\prod_{i=1}^N p(y_i | x_i, w)\n",
    "$$\n",
    "We can learn the parameter $w$ by maximizing this probability (i.e. we find the maximum likelihood estimator) or equivalently minimizing $J(w) = -\\log p(\\vec{y} | X, w)$.  Derive the loss function $J(w)$ step-by-step and implement it using the skeletal outline below.  In your implementation you may use the convention that $0 \\log 0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the loss function using the formula you derived.  This code does not need to be vectorized and you\n",
    "may use the logistic_unit function.\n",
    "\n",
    "Input:\n",
    "    w : weight vector: numpy array of shape (d+1,) (remember that x_0 = 1)\n",
    "    X : dataset features: numpy array of shape (n,d)\n",
    "    y : dataset targets: numpy array of shape (n,) contains only 0's or 1's\n",
    "\n",
    "Return:\n",
    "    J : loss of w given X,y: float\n",
    "\"\"\"\n",
    "def sigmoid(X):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def net_input(w, X):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(X, w)\n",
    "    \n",
    "def loss(w, X, y):\n",
    "    h = sigmoid(net_input(w,X))\n",
    "    J = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making data (Nothing to do here)\n",
    "\n",
    "The following method generates random data to test your algorithm on.  **DO NOT CHANGE THE METHOD!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following method generates a random dataset.  DO NOT ALTER THIS METHOD.\n",
    "def make_data(n_samples = 500):\n",
    "    # Generate data features.\n",
    "    X1 = mvn.rvs(mean = np.array([1.1, 0]), cov = 0.2*np.eye(2), size = n_samples//2)\n",
    "    X2 = mvn.rvs(mean = np.array([-1.1, 0]), cov = 0.2*np.eye(2), size = n_samples - n_samples//2)\n",
    "    # Append data labels and combine.\n",
    "    X1 = np.hstack( (X1, np.ones((X1.shape[0], 1))))\n",
    "    X2 = np.hstack( (X2, np.zeros((X2.shape[0], 1))))\n",
    "    X = np.vstack([X1, X2])\n",
    "    # Randomly permute data.\n",
    "    np.random.shuffle(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) (5 pts) Splitting the data for training and testing\n",
    "\n",
    "Now we'll actually learn a logistic regression model for some synthetic data.\n",
    "\n",
    "First split the dataset into a training, validation, and test set.  Use a 40/40/20 split (roughly 40/40/20 is fine).  You do not need to use a random splitting, although in practice it is usually a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first generate some fake data.\n",
    "np.random.seed(2) # Don't change the random seed.\n",
    "data = make_data()\n",
    "X = data[:,:-1] # Features\n",
    "y = data[:,-1]  # Labels\n",
    "\n",
    "# We'll also augment the data so that x_0 = 1 for the intercept term.\n",
    "X = np.append(np.ones((len(X), 1)), X, axis = 1)\n",
    "\n",
    "\n",
    "# TO DO:\n",
    "## Your part starts here.\n",
    "\n",
    "## Ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) (20 pts)  Training the model\n",
    "\n",
    "Now use your gradient descent function to learn the parameters $w$ using `5000` iterations.  You may choose the learning rate and initial parameters $w_0$ for this problem.  Compare both the fixed learning rate and variable learning rate gradient descents side by side (i.e. 2 subplots) by computing the loss after every `m=10` iterations.  There are 4 things to do for this problem.\n",
    "1. Set the learning rate and initial points.\n",
    "2. Implement the loss function $J(w)$.\n",
    "3. Update the parameters using the two gradient descent methods.\n",
    "4. Compute the loss after every 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5000        # Lots of iterations to see the training error go down.\n",
    "m = 10                   # Get the loss every m iterations.\n",
    "\n",
    "# TO DO: Set the learning rate.\n",
    "lr = ?                # You choose this value, try something small.\n",
    "\n",
    "\n",
    "\n",
    "## Store the loss values in these arrays.\n",
    "# Fixed learning rate.\n",
    "train_loss_fixed = np.zeros(iterations//m)\n",
    "val_loss_fixed = np.zeros(iterations//m)\n",
    "\n",
    "# Variable learning rate.\n",
    "train_loss_var = np.zeros(iterations//m)\n",
    "val_loss_var = np.zeros(iterations//m)\n",
    "\n",
    "\n",
    "# TO DO: Set the initial values, either randomly or some fixed value.\n",
    "w1 = ?\n",
    "w2 = ?\n",
    "\n",
    "# TO DO: Write the loss function as a function of the parameter only.\n",
    "J = ?\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TO DO: Get the updated parameters from gradient descent with a fixed learning rate.\n",
    "    w1 = ?\n",
    "    # TO DO: Get the updated parameters from gradient descent with a variable learning rate.\n",
    "    # Hint: You may either modify the gd_variable function above or call gd_fixed with a different learning rate.\n",
    "    w2 = ?\n",
    "    \n",
    "    # Only compute the loss every m iterations.\n",
    "    if np.mod(iterations, m) == 0: \n",
    "        # TO DO: Compute the training and validation loss of the parameters found with the fixed learning rate.\n",
    "        train_loss_fixed[i//m] = ?\n",
    "        val_loss_fixed[i//m] = ?\n",
    "        # TO DO: Compute the training and validation loss of the parameters found with a variable learning rate.\n",
    "        train_loss_var[i//m] = ?\n",
    "        val_loss_var[i//m] = ?\n",
    "\n",
    "        \n",
    "## Plotting starts here.  Nothing to implement.\n",
    "its = np.arange(1, iterations + 1, m)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 4))\n",
    "\n",
    "ax1.semilogy(its, train_loss_fixed, 'b-', label = 'Train')\n",
    "ax1.semilogy(its, val_loss_fixed, 'r-', label = 'Val')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('GD with Fixed Learning Rate')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.semilogy(its, train_loss_var, 'b-', label = 'Train')\n",
    "ax2.semilogy(its, val_loss_var, 'r-', label = 'Val')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('GD with Variable Learning Rate')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) (10 pts) Evaluating the model on the test set\n",
    "\n",
    "Finally, we have trained our models and are ready to evaluate them on the test set.  For binary classification one way to check our classifier is to make a *confusion matrix* of our predictions.\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "\\text{Predict 0, Actual 0} & \\text{Predict 0, Actual 1}\\\\\n",
    "\\text{Predict 1, Actual 0} & \\text{Predict 1, Actual 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The diagonal elements are the number of samples that are correctly classified.\n",
    "\n",
    "Use both trained models (fixed and variable learning rates) to classify samples in the test set according to whether $h(x_i; w) \\ge 0.5$ or not and print the confusion matrices.  Also print the accuracy rate which is just the percentage of correctly classified examples for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.zeros((2, 2)) # Confusion matrix for fixed learning rate.\n",
    "C2 = np.zeros((2, 2)) # Confusion matrix for variable learning rate.\n",
    "N = len(X_test)       # Number of test samples.\n",
    "\n",
    "\n",
    "## TO DO STARTS HERE:\n",
    "\n",
    "\n",
    "# Compute the accuracy         \n",
    "acc1 = ?\n",
    "acc2 = ?\n",
    "\n",
    "# TO DO ENDS HERE.\n",
    "print(\"C_1 = \")\n",
    "print(C1)\n",
    "print(\"C_2 = \")\n",
    "print(C2)\n",
    "print('Fixed Learning Rate Accuracy    = {:0.1f}%'.format(100 * acc1))\n",
    "print('Variable Learning Rate Accuracy = {:0.1f}%'.format(100 * acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Loss functions (10 pts total)\n",
    "\n",
    "We have already seen two examples of loss functions: the squared loss and the logistic loss.  In this problem we will look at two more important examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) $L^p$ loss\n",
    "\n",
    "Similar to the squared loss, we can minimize the $L^p$ loss defined as\n",
    "$$\n",
    "L(w) = \\frac{1}{n} \\sum_{i=1}^n |y_i - w^T x_i |^p\n",
    "$$\n",
    "for $p > 1$.  Whenever, $p = 1$ the problem is no longer differentiable so we cannot take the gradient.  Derive the gradient of this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (5 pts) Regularized loss\n",
    "\n",
    "Sometimes it's a good idea to add an additional penalty to the weights.  An example, which appears in ridge regression, is\n",
    "$$\n",
    "L(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i - w^T x_i)^2 + \\sum_{j=1}^d w_j^2\n",
    "$$\n",
    "Derive the gradient of this loss function.  How does the minimizer $w$ of the regularized loss compare to the minimzer of the squared loss?  (A heuristic explanation is fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
